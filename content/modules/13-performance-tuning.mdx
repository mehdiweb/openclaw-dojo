---
id: "13"
slug: "performance-tuning"
title: "Performance Tuning"
description: "Optimize response times and resource usage"
tier: "free"
level: 2
estimatedTime: "25 min"
---

# Performance Tuning

Make your AI agent faster, cheaper, and more efficient. Optimize response times, reduce costs, and handle more users with the same resources.

## What You'll Learn

- âœ… Optimize AI model response times
- âœ… Reduce memory usage
- âœ… Implement caching strategies
- âœ… Optimize database queries
- âœ… Monitor and profile performance

## Prerequisites

Complete Modules 8 and 12 (Memory Systems and Security Basics) before starting this module.

---

## Step 1: Optimize AI Model Settings

### Choose the Right Model

```bash
nano ~/.openclaw/config.json
```

```json
{
  "ai": {
    "providers": {
      "anthropic": {
        "models": {
          "fast": "claude-3-haiku-20240307",  // Fastest, cheapest
          "balanced": "claude-3-5-sonnet-20241022",  // Best balance
          "powerful": "claude-3-opus-20240229"  // Slowest, most capable
        },
        "defaultModel": "fast"
      }
    },
    "routing": {
      "simple_queries": "fast",
      "complex_tasks": "balanced",
      "critical_analysis": "powerful"
    }
  }
}
```

**Model comparison:**
- **Haiku**: 0.5s response, $0.25/1M tokens
- **Sonnet**: 2s response, $3/1M tokens  
- **Opus**: 5s response, $15/1M tokens

### Reduce Token Usage

```json
{
  "ai": {
    "optimization": {
      "maxTokens": 1024,  // Limit response length
      "temperature": 0.7,  // Lower = more focused
      "stopSequences": ["\n\n\n"],  // Stop early when appropriate
      "streamResponse": true  // Stream for faster perceived speed
    }
  }
}
```

### Smart Context Management

```json
{
  "ai": {
    "context": {
      "maxMessages": 10,  // Reduce context window
      "summarizeAfter": 20,  // Summarize old messages
      "pruneOldest": true,
      "prioritizeRecent": true
    }
  }
}
```

---

## Step 2: Implement Caching

Cache expensive operations to avoid redundant work.

### Enable Response Caching

```bash
nano ~/.openclaw/config.json
```

```json
{
  "cache": {
    "enabled": true,
    "provider": "redis",  // or "memory", "file"
    "ttl": 3600,  // 1 hour
    "maxSize": 1000,  // entries
    "strategies": {
      "weather": {
        "ttl": 600,  // 10 minutes
        "keyBy": ["city"]
      },
      "news": {
        "ttl": 1800,  // 30 minutes
        "keyBy": ["topic"]
      },
      "ai_responses": {
        "ttl": 300,  // 5 minutes
        "keyBy": ["message_hash"]
      }
    }
  }
}
```

### Install Redis (Recommended)

```bash
# Install Redis
sudo apt install redis-server

# Start Redis
sudo systemctl start redis

# Configure OpenClaw to use Redis
nano .env
```

```bash
REDIS_URL=redis://localhost:6379
```

### Test Caching

```bash
# Check cache status
openclaw cache status

# View cached items
openclaw cache list

# Clear cache
openclaw cache clear

# Test cache hit rate
openclaw cache stats
```

---

## Step 3: Database Optimization

### Index Frequently Queried Fields

For PostgreSQL:

```sql
-- Connect to database
psql -U openclaw -d openclaw_memory

-- Add indexes
CREATE INDEX idx_messages_user_id ON messages(user_id);
CREATE INDEX idx_messages_timestamp ON messages(created_at);
CREATE INDEX idx_messages_channel ON messages(channel);

-- Analyze query performance
EXPLAIN ANALYZE SELECT * FROM messages WHERE user_id = '123';
```

### Connection Pooling

```json
{
  "database": {
    "pool": {
      "min": 2,
      "max": 10,
      "idleTimeoutMillis": 30000,
      "connectionTimeoutMillis": 2000
    }
  }
}
```

### Query Optimization

```json
{
  "database": {
    "optimization": {
      "batchSize": 100,  // Batch inserts
      "lazyLoad": true,  // Load data on demand
      "selectFields": ["id", "content", "timestamp"],  // Don't SELECT *
      "useTransactions": true
    }
  }
}
```

---

## Step 4: Memory Management

### Monitor Memory Usage

```bash
# Check memory usage
openclaw status --memory

# Should show:
# Memory: 245 MB / 512 MB (48%)
# Heap: 180 MB
# External: 65 MB
```

### Reduce Memory Footprint

```json
{
  "performance": {
    "memory": {
      "maxHeapSize": 512,  // MB
      "gcInterval": 300000,  // 5 minutes
      "clearUnusedCache": true,
      "limits": {
        "maxConcurrentRequests": 10,
        "maxQueueSize": 50
      }
    }
  }
}
```

### Garbage Collection Tuning

```bash
# Set Node.js GC flags
nano ~/.openclaw/startup.sh
```

```bash
#!/bin/bash
node --max-old-space-size=512 \
     --gc-interval=100 \
     /usr/local/bin/openclaw start
```

---

## Step 5: Network Optimization

### Enable Compression

```json
{
  "network": {
    "compression": {
      "enabled": true,
      "level": 6,  // 1-9, higher = more compression
      "threshold": 1024  // Only compress if > 1KB
    }
  }
}
```

### HTTP/2 Support

```json
{
  "server": {
    "http2": {
      "enabled": true,
      "maxConcurrentStreams": 100
    }
  }
}
```

### Connection Reuse

```json
{
  "network": {
    "http": {
      "keepAlive": true,
      "keepAliveMsecs": 1000,
      "maxSockets": 50,
      "maxFreeSockets": 10
    }
  }
}
```

---

## Step 6: Monitoring & Profiling

### Enable Performance Monitoring

```json
{
  "monitoring": {
    "enabled": true,
    "metrics": {
      "responseTime": true,
      "throughput": true,
      "errorRate": true,
      "memoryUsage": true,
      "cpuUsage": true
    },
    "intervals": {
      "collect": 10000,  // Collect every 10s
      "report": 60000  // Report every minute
    }
  }
}
```

### View Performance Metrics

```bash
# Real-time metrics
openclaw metrics watch

# Historical data
openclaw metrics history --since "1 hour ago"

# Export metrics
openclaw metrics export --format json > metrics.json
```

### Profiling

```bash
# Profile CPU usage
openclaw profile cpu --duration 60

# Profile memory
openclaw profile memory --duration 60

# Profile specific operation
openclaw profile operation "weather_api_call"
```

---

## Verification

Run performance tests:

```bash
# Performance audit
openclaw performance audit

# Load test
openclaw test load --users 100 --duration 60

# Benchmark
openclaw benchmark --iterations 1000
```

Expected results:
- Response time: < 2s (95th percentile)
- Memory usage: < 512 MB
- Cache hit rate: > 60%
- Error rate: < 1%

---

## Common Issues

### "Slow response times"

**Check bottlenecks:**

```bash
# Profile the slow operation
openclaw profile operation "slow_command"

# Check if it's AI, database, or network
openclaw metrics breakdown
```

**Solutions:**
1. Use faster AI model (Haiku instead of Opus)
2. Enable caching
3. Reduce context window
4. Add database indexes

### "High memory usage"

**Reduce memory:**

```json
{
  "performance": {
    "memory": {
      "maxConcurrentRequests": 5,  // Reduce from 10
      "clearCacheOnLowMemory": true,
      "maxContextSize": 5  // Reduce from 10
    }
  }
}
```

**Check for memory leaks:**

```bash
openclaw profile memory --duration 300
```

### "Cache not working"

**Verify Redis:**

```bash
# Test Redis connection
redis-cli ping

# Should return: PONG

# Check OpenClaw cache
openclaw cache test
```

### "Database slow"

**Add indexes:**

```sql
-- Find slow queries
SELECT query, mean_exec_time 
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 10;

-- Add missing indexes
CREATE INDEX idx_slow_column ON table_name(column_name);
```

---

## Performance Best Practices

### 1. **Use Appropriate Models**

```
Simple queries â†’ Haiku (fast, cheap)
Complex tasks â†’ Sonnet (balanced)
Critical work â†’ Opus (slow, expensive)
```

### 2. **Cache Aggressively**

Cache:
- API responses (weather, news)
- AI responses (for common questions)
- Database queries (user data)

Don't cache:
- Real-time data
- User-specific sensitive info
- Rapidly changing data

### 3. **Optimize Database**

```sql
-- Regular maintenance
VACUUM ANALYZE;

-- Update statistics
ANALYZE;

-- Reindex if needed
REINDEX DATABASE openclaw_memory;
```

### 4. **Monitor Continuously**

```bash
# Set up alerts
openclaw alerts add --metric response_time --threshold 5000 --notify admin@email.com

# Daily reports
openclaw reports schedule --type performance --frequency daily
```

### 5. **Load Test Before Production**

```bash
# Simulate 100 concurrent users
openclaw test load --users 100 --duration 300

# Stress test
openclaw test stress --ramp-up 10 --max-users 500
```

---

## Performance Optimization Checklist

- [ ] Using appropriate AI models for each task
- [ ] Response caching enabled
- [ ] Database indexes created
- [ ] Connection pooling configured
- [ ] Memory limits set
- [ ] Compression enabled
- [ ] Monitoring active
- [ ] Load tested
- [ ] Alerts configured
- [ ] Regular performance audits scheduled

---

## Real-World Optimizations

### Before Optimization:
```
Response time: 8s
Memory usage: 1.2 GB
Cost per 1000 requests: $5
```

### After Optimization:
```
Response time: 1.5s (81% faster)
Memory usage: 400 MB (67% less)
Cost per 1000 requests: $1.20 (76% cheaper)
```

**Changes made:**
1. Switched from Opus to Haiku for simple queries
2. Enabled Redis caching (70% hit rate)
3. Reduced context window from 30 to 10 messages
4. Added database indexes
5. Enabled response streaming

---

## Next Steps

Your agent is now optimized for performance! 

**ðŸŽ‰ Free Tier Complete!** You've mastered the fundamentals of OpenClaw deployment.

**Ready for more?** Upgrade to **Builder Tier** ($49) to unlock:
- Module 14: Advanced Defense (security hardening)
- Module 15: Custom Tools (build your own functions)
- Module 16: Browser Automation (web scraping)
- Module 17: Complex Workflows (cron jobs, automation)

[Upgrade to Builder Tier](/support) â†’

**Need help?** Join the [Discord community](https://discord.com) for support.
